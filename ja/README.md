# スタンフォード大学 CME 295「Transformer と大規模言語モデル」のチートシート
対応言語：[English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) - [Español](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/es) - [Français](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/fr) - **日本語**

## 目的
このリポジトリは、スタンフォード大学 CME 295「Transformer と大規模言語モデル」で扱う重要概念を一箇所にまとめることを目的としています。内容は以下のとおりです。
- **Transformer**：Self-Attention、構造、派生モデル、最適化手法（Sparse Attention・低ランク Attention・Flash Attention）
- **大規模言語モデル**：プロンプト、ファインチューニング（SFT・LoRA）、プリファレンスチューニング、最適化手法（混合エキスパート・蒸留・量子化）
- **応用**：LLM-as-a-judge、RAG、エージェント、推論モデル（DeepSeek-R1 によるトレーニング時とテスト時のスケーリング）

## 内容
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/ja/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-ja.png" alt="Illustration" width="600px"/></a>

## 講義テキスト
この VIP Cheatsheet は、『Super Study Guide: Transformer と大規模言語モデル』という書籍の概要です。この書籍では、250 ページにわたって約 600 点のイラストを用い、以下の概念を詳細に解説しています。詳細は https://superstudy.guide をご覧ください。

## 講義ウェブサイト
[cme295.stanford.edu](https://cme295.stanford.edu/)

## 著者
[アフシン・アミディ](https://twitter.com/afshinea)（パリ中央工科大学、MIT）

[シェルビン・アミディ](https://twitter.com/shervinea)（パリ中央工科大学、スタンフォード大学）

## 訳者
中井 喜之
