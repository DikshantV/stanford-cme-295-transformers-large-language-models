# Guía del curso sobre Transformers y Grandes Modelos de Lenguaje para el curso CME 295 de Stanford
Disponible en: [English](https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/tree/main/en) -  **Español**

## Objetivo
Este repositorio tiene como objetivo resumir en un solo lugar todas las nociones importantes que se cubren en el curso de CME 295 Transformers & Large Language Models de Stanford. Incluye:
- **Transformers**: auto-atención, arquitectura, variantes, técnicas de optimización (atención dispersa, atención de bajo rango, atención flash)
- **LLMs**: prompting, ajuste fino (SFT, LoRA), ajuste de preferencias, técnicas de optimización (mezcla de expertos, destilación, cuantización)
- **Applications**: LLM como juez, RAG, agentes, modelos de razonamiento (escalado en tiempo de entrenamiento y en tiempo de prueba de DeepSeek-R1)

## Contenido
### VIP Cheatsheet
<a href="https://github.com/afshinea/stanford-cme-295-transformers-large-language-models/blob/main/es/cheatsheet-transformers-large-language-models.pdf"><img src="https://cme295.stanford.edu/cheatsheet-es.png" alt="Illustration" width="600px"/></a>

## Libro de texto
Esta VIP cheatsheet ofrece un resumen del contenido del libro "Super Study Guide: Transformers & Large Language Models”, que incluye ~600 ilustraciones a lo largo de 250 páginas y profundiza en los siguientes conceptos. Puedes encontrar más detalles en https://superstudy.guide.

## Sitio web de la clase
[cme295.stanford.edu](https://cme295.stanford.edu/)

## Autores
[Afshine Amidi](https://twitter.com/afshinea) (Ecole Centrale Paris, MIT) y [Shervine Amidi](https://twitter.com/shervinea) (Ecole Centrale Paris, Stanford University)

## Traductores
Steven Van Vaerenbergh (Universidad de Cantabria) y Lara Lloret Iglesias (Instituto de Física de Cantabria - CSIC/UC)
